#!/usr/bin/env ruby 

# command_runner.rb NUMBER_OF_CPUS

# This script is only run on the master node of the cluster as the "elasticwulf" user from within the NFS home directory "/home/elasticwulf/".  It fetches input & code from s3, runs the job command, and uploads outputs to S3.  The job command it runs will typically be a bash script containing MPI commands run across the entire cluster using the input data fetched from S3 which is available to all nodes via NFS.

require 'rubygems'
require 'activeresource'
require 'right_aws'
require 'net/http'

# cluster_config.yml should contain the following variables (autogenerated during cluster configuration):
# aws_access_key_id: $aws_access_key_id
# aws_secret_access_key: $aws_secret_access_key
# admin_user: $admin_user
# admin_password: $admin_password
# rest_url: $rest_url
# job_id: $job_id

CPU_COUNT=ARGV[0]
ENV['CPU_COUNT'] = CPU_COUNT

CLUSTER_CONFIG = YAML.load_file("/home/elasticwulf/cluster_config.yml")
puts "job id: " + CLUSTER_CONFIG['job_id'].to_s

# Create an ActiveResource connection to the Elasticwulf REST web service
class Job < ActiveResource::Base
  self.site = CLUSTER_CONFIG['rest_url']
  self.user = CLUSTER_CONFIG['admin_user']
  self.password = CLUSTER_CONFIG['admin_password']
end

job = Job.find(CLUSTER_CONFIG["job_id"].to_i)
s3 = RightAws::S3Interface.new(CLUSTER_CONFIG['aws_access_key_id'],
            CLUSTER_CONFIG['aws_secret_access_key'], {:multi_thread => true})

############################
# Fetch files from s3 to local working directory
job.put(:updateprogress, :progress => 'downloading inputs from S3')
puts job.progress
@input_files = job.input_files.split   
# TODO need to loop over inputs and fetch from s3.  will assume a prefix of s3:// 
 
# TODO: pre-verify s3:// prefix in REST service submissions
      
# fetch http://datawrangling.s3.amazonaws.com/colbert_2.gif   
foo = File.new('./colbert_2.gif', File::CREAT|File::RDWR) 
rhdr = s3.get('datawrangling', 'colbert_2.gif') do |chunk| foo.write(chunk) end
foo.close   

############################
# Run job command and wait for completion, periodically updating progress
job.put(:nextstep)  # Signal REST service, job state will transition from configuring cluster -> running_job
puts job.state

# TODO: kick off command as a thread or child process with popen4
system(job.commands)

############################
# Upload job output files to S3
job.put(:updateprogress, :progress => 'uploading outputs to S3')
puts job.progress

# here I use the datawrangling bucket
s3.put('datawrangling', 'cluster_mpi_smoketest.txt',  File.open('/home/elasticwulf/cluster_mpi_smoketest.txt'))

# TODO: right now we parse space delimited list of output files (assuming they are created in working directory)
# better convention might be to accept a pattern, i.e. foo*, which will grab all files matching that pattern.
# we can do an ls on that pattern and then iterate over the results.

# Convention may need to be for the command to write all files to a directory (relative to /home/elasticwulf): /home/elasticwulf/output

# TODO: save logs to s3 before shutting down cluster

# job.put(:nextstep)  # Signal REST service, job state will transition from running_job -> shutdown_requested

#############################

# In Case of error, stop job and ping server with error message

# TODO: wrap the main section in a try/rescue block which puts 'error' for the job state in case of failure
# # PUT an update by invoking the 'error' REST method, 
# puts "triggering error..."
# job.put(:error, :error_message => 'some horrible error message')
# job = Job.find(CLUSTER_CONFIG["job_id"].to_i)
# puts job.state





